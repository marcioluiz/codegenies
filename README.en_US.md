# CodeGenies: Team of SLMs for Software Development in Task Graph

This is a project developed with the purpose of creating a natural language-based platform that allows development teams to work efficiently, integrating artificial intelligence to automate architectural design, code development and testing activities. 
The project makes use of small language models (or SLMs).


## Requirements

- Python 3.11 (Tested with Python 3.11.9)
- Conda or Miniconda installed
- SLM agents installed locally with Ollama (Codegemma, Mistral, Llama-3)
- Framework [Langchain](https://github.com/langchain-ai/langchain) to execute prompts on the SLMs when traversing the task graph created with the [Langgraph](https://github.com/langchain-) extension ai/langgraph)

## Project Configuration

### Ollama Installation

1. **Download and install Ollama:**
   - Visit the [Ollama official website](https://ollama.ai) and follow the instructions to download and install the software.

2. **Configure the Models:**
   - You will need the following templates installed and configured in Ollama:
     - [phi3:14b-medium-128k-instruct-q4_K_M](https://ollama.com/library/phi3:14b-medium-128k-instruct-q4_K_M) (Analyst)
     - [codegemma:7b-instruct-v1.1-q4_K_M](https://ollama.com/library/codegemma:7b-instruct-v1.1-q4_K_M) (Developer)
     - [llama3:8b-instruct-q4_K_M](https://ollama.com/library/llama3:8b-instruct-q4_K_M) (Squad Leader)
   - If you wish, simply change these names to the models you prefer in the `main.py` file. Of course, you should download the homologous models from the [Ollama Library](https://ollama.com/library/).

3. **Start the Templates:**
   - Ensure templates are running and accessible so agents can communicate with them. Check the ports used and adjust your firewall or router as necessary.

### Project Configuration

To configure this project, follow the steps below:

1. **Create a Project Folder:**
   - Create a folder with the name of your project.

2. **Create a Conda Environment for the Project:**
   - Create a conda environment with the name of your project:
   ```
      conda create -n codegenies python=3.11
   ```
   - Run the environment and check the execution:
   ```
      conda activate codegenies
      conda info
   ```

3. **Install project dependencies:**
   - Create a conda environment with the name of your project:
   ```
      pip install -r requirements.txt
   ```

4. **Configure the `project.properties` File:**
   - Use a text editor to edit the file called `project.properties` with the following content:
     ```
     # Project Title and Description
     title=
     description=
     author=

     # Complete technical details of the Project
     technical_details=

     # Technologies used in project layers
     backend_technology=
     frontend_technology=
     ```

5. **Start External Services:**
   - Make sure models are installed locally [via Ollama Library](https://ollama.com/library/) or from external services (like [Hugging Face](https://huggingface.co/models?sort= downloads&search=gguf)) are running and accessible to communicate with the project. Check the ports used and adjust your firewall or router as necessary.

6. **Run the Main Script:**
   - Run the `main.py` file.
   - Example: `python main.py`

7. **Configure New Agents:**
   - Create new agents in the `agents` folder as needed by your project to perform different tasks, such as generating structure files, folders or code files.

## Project Execution

To run the project, follow these steps:

1. **Start External Services:**
   - Make sure the models installed locally (via Ollama) or on external services (like Hugging Face) are running.

2. **Run the Main Script:**
   - Run the `main.py` file passing the name of your project and the location of the `project.properties` file as parameters.
   - The system will begin generating initial reports for each of the agents and will begin creating the structure of folders and codes according to the respective activity backlogs generated by the analyst.

3. **Work on Codes:**
   - Developers will work in their separate folders in the project, implementing their codes according to the backlogs.

4. **Test the Code:**
   - The tester must perform the tests and add them to their separate folder, also within the project.
   - The team must work together to carry out tests on the code produced and correct any errors found.

5. **Generate Reports:**
   - When ready, the system generates a complete report with agent information and the results of the tests carried out by the tester.

6. **Update the Project:**
   - You can return to the beginning of the project to update information, backlogs and reports, generating a new file structure as needed.

## Project Folder Structure

```
The project folder structure will be organized as follows:
project_name/
├── agents/
│   ├── analyst.py
│   ├── squad_leader.py
│   ├── developer_backend.py
│   ├── developer_frontend.py
│   ├── tester.py
|   └── prompt_templates/
|        ├── analyst_prompts.py
│        ├── squad_leader_prompts.py
│        └── developer_prompts.py
├── build/
│   ├── dev/
│   │   ├── desenvolvedor_backend/
│   │   ├── desenvolvedor_frontend/
│   │   └── tester/
│   ├── reports/
│   │   ├── relatorio_geral_do_projeto.txt
│   │   ├── backlog_de_tarefas_de_backend.txt
│   │   ├── backlog_de_tarefas_de_frontend.txt
│   │   └── backlog_de_tarefas_de_testes.txt
│   └── README.md
├── graph.py
├── LICENSE (GPL-3.0 License)
├── main.py
├── project.properties
├── README.en_US.md
├── README.md
└── requirements.txt
```

## Licensing Information

This project is released under the GNU General Public License v3.0 (GPL-3.0). This means that you are free to use, modify and distribute this code as long as you respect the terms of the GPL license. The full license text can be found in the LICENSE file included with the project.

## Contribution

Feel free to contribute to this project. If you have any questions or encounter problems, please open an issue or submit a pull request.

---

**Enjoy the integration of artificial intelligence into your development teams with CodeGenies!**